{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f9fc04-c553-4f3f-971a-ecc6e6dde5c9",
   "metadata": {},
   "source": [
    "# Neural Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8360c46d-8ed8-4446-b8bb-ca3aabf1f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ffe08bb-32c3-4811-b281-dacb5381a8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset_name = \"label_top\"\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 3     # this parameter defines how many time the network sees each item of the dataset\n",
    "lr = 1e-3          # this parameter defines how much the parameters of the network are updated after each step\n",
    "batch_size=1024    # this parameter defines how many samples are run through the network in each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28ebc39e-4bc3-40cc-a34c-945e0c53c6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This class loads data from a pandas dataframe\n",
    "    \"\"\"\n",
    "    def __init__(self, filename_features: str, filename_labels: str):\n",
    "        \"\"\"\n",
    "        Initialize the dataset class\n",
    "        :param filename: The filename of the CSV file\n",
    "        \"\"\"\n",
    "        self.df_features = pd.read_pickle(filename_features, compression='gzip').astype(float).to_numpy()\n",
    "        df_labels = pd.read_pickle(filename_labels, compression='gzip').astype(int)\n",
    "        self.n_features = self.df_features.shape[1]\n",
    "        self.n_classes = df_labels.shape[1]\n",
    "        self.df_labels = (df_labels * np.arange(df_labels.shape[1])).sum(axis=1).to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        This function returns the total number of items in the dataset.\n",
    "        We are using a pandas data frame in this dataset which has an attribut named shape.\n",
    "        The first dimension of shape is equal to the number of items in the dataset.\n",
    "        :return: The number of rows in the CSV file\n",
    "        \"\"\"\n",
    "        return self.df_features.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        This function returns a single tuple from the dataset.\n",
    "        :param idx: The index of the tuple that should be returned.\n",
    "        :return: Tuple of an x-value and a y-value\n",
    "        \"\"\"\n",
    "        return self.df_features[idx], self.df_labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "134381d6-de4e-4675-bbd0-73b48a5db39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationNetwork(nn.Module):\n",
    "    def __init__(self, n_features: int, n_classes: int, hidden_dim=128, f_activation=nn.ReLU):\n",
    "        \"\"\"\n",
    "        Here we define the layers of our neural network.\n",
    "        \"\"\"\n",
    "        super(ClassificationNetwork, self).__init__()\n",
    "        # Our data has four features, so the first linear layer has to have four input dimensions.\n",
    "        self.layer1 = nn.Linear(n_features, hidden_dim)\n",
    "        # The first hidden layer need to have the same input dimension as layer1 has outputs. \n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # We have three different classes in out data, so the last linear layer must have 3 output dimensions.\n",
    "        self.layer3 = nn.Linear(hidden_dim, n_classes)\n",
    "        self.activation = f_activation()\n",
    "        # The outputs of the last linear layer need to be mapped to a probability function.\n",
    "        # This can be done by running the vectors through a softmax function.\n",
    "        self.classification = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward function takes a data vector and runs it through the layers of our neural network.\n",
    "        :return: The forward function returns a vector of size 3 which contains the\n",
    "            probabilities for all three classes for a given data vector.\n",
    "        \"\"\"\n",
    "        # Run the input through the first linear layer and then through the activation function.\n",
    "        x = self.activation(self.layer1(x))\n",
    "        # Run the outputs of layer 1 through layer 2.\n",
    "        x = self.activation(self.layer2(x))\n",
    "        # Run the outputs of layer 2 through the third linear layer and then through the softmax classification function.\n",
    "        x = self.classification(self.layer3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c35046c3-2e28-46fb-afab-30d462d72ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NetworkDataset(\"features_final.pkl.gz\", f\"{dataset_name}_ohe.pkl.gz\")\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "744fd890-3a75-419f-8f1c-9ce0aebf985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have defined the network class we need to create an instance of it\n",
    "net = ClassificationNetwork(n_features=dataset.n_features, n_classes=dataset.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b00aa26-e93d-4180-a10d-31de6d97205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(net, dataloader):\n",
    "    \"\"\"\n",
    "    This function computes the accuracy of the neural network by sampling data from a\n",
    "    data loader, running it through the network and computing the percentage of correct predictions.\n",
    "    :param net: The neural network\n",
    "    :param dataloader: A DataLoader instance\n",
    "    \"\"\"\n",
    "    # torch.no_grad means that no gradients should be computed when running data through the network.\n",
    "    # When we run test data through the network this should not have an effect on our training, that is\n",
    "    # why we don't want to compute gradients here.\n",
    "    with torch.no_grad():\n",
    "        X_test, y_test = next(iter(dataloader))\n",
    "        y_pred = net(X_test.to(torch.float32))\n",
    "        correct = (torch.argmax(y_pred, dim=1) == y_test).type(torch.float32)\n",
    "        return correct.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64593077-8e3c-4cf5-b12b-c67e89556c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before training: 0.0341796875\n"
     ]
    }
   ],
   "source": [
    "# Let's check the accuracy before training the network\n",
    "print(\"Accuracy before training:\", get_accuracy(net, dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8ad0677-523a-479c-ad0a-51f95939e2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 1.4602, Accuracy on test data: 0.6845703125\n",
      "Epoch [2/3], Loss: 1.4489, Accuracy on test data: 0.6953125\n",
      "Epoch [3/3], Loss: 1.4867, Accuracy on test data: 0.6982421875\n"
     ]
    }
   ],
   "source": [
    "# This is our loss function. Which one do we need for classification: MSELoss or CrossEntropyLoss?\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# This is the algorithm used for optimizing our neural network parameters.\n",
    "optimizer = optim.AdamW(net.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, Y in dataloader:        \n",
    "        # Forward pass\n",
    "        outputs = net(X.to(torch.float32))\n",
    "        \n",
    "        # Compute the difference between the true labels and the predicted labels\n",
    "        loss = criterion(outputs, Y.to(torch.long))\n",
    "    \n",
    "        # First reset the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Then compute the new gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # And finally perform the backpropagation step\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Print some metrics about the learning progress\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        accuracy = get_accuracy(net, dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy on test data:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55397a03-70a8-4214-9a40-dad6db0b7fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
